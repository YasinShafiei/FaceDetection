{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9cd045e",
   "metadata": {},
   "source": [
    "## Face detection\n",
    "Detect, classify and localize face using Deep learning and CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460dc2c1",
   "metadata": {},
   "source": [
    "### Import libraries and define vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0407f201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f996220",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 16\n",
    "DATA_PATH = os.path.join(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7cdcc4",
   "metadata": {},
   "source": [
    "### Load data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a408a247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(image):\n",
    "    \"\"\"\n",
    "    This function will load and decode images\n",
    "    \"\"\"\n",
    "    # read and decode\n",
    "    load = tf.io.read_file(image)\n",
    "    decoded = tf.io.decode_jpeg(load)\n",
    "    \n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df5edb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(label_path):\n",
    "    \"\"\"\n",
    "    This function will load labels and return class and bboxes\n",
    "    \"\"\"\n",
    "    # read the label file (.json format)\n",
    "    with open(label_path.numpy(), 'r', encoding = \"utf-8\") as f:\n",
    "        label = json.load(f)\n",
    "        \n",
    "    # get the bounding box coordinates and class\n",
    "    label_class = [label['class']]\n",
    "    label_bbox = label['bbox']\n",
    "    \n",
    "    return label_class, label_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4276ffd0",
   "metadata": {},
   "source": [
    "### Create dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae43b3d0",
   "metadata": {},
   "source": [
    " We have to load our data (images and labels), then create our final dataset using `tensorflow`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde918c",
   "metadata": {},
   "source": [
    "1. Load images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb2e444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train images\n",
    "train_images = tf.data.Dataset.list_files('data/train/images/*.jpg', shuffle=False)\n",
    "train_images = train_images.map(load_images)\n",
    "train_images = train_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "train_images = train_images.map(lambda x: x/255)\n",
    "\n",
    "# load validation images\n",
    "val_images = tf.data.Dataset.list_files('data/val/images/*.jpg', shuffle=False)\n",
    "val_images = val_images.map(load_images)\n",
    "val_images = val_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "val_images = val_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0802bb",
   "metadata": {},
   "source": [
    "2. Load labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44c74f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train labels\n",
    "train_labels = tf.data.Dataset.list_files('data/train/labels/*.json', shuffle=False)\n",
    "train_labels = train_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))\n",
    "\n",
    "# load validation labels\n",
    "val_labels = tf.data.Dataset.list_files('data/val/labels/*.json', shuffle=False)\n",
    "val_labels = val_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8474f62c",
   "metadata": {},
   "source": [
    "3. Create final dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3fe3b670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train dataset\n",
    "train_dataset = tf.data.Dataset.zip((train_images, train_labels))\n",
    "train_dataset = train_dataset.shuffle(5000)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c829303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation dataset\n",
    "val_dataset = tf.data.Dataset.zip((val_images, val_labels))\n",
    "val_dataset = val_dataset.shuffle(5000)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a5379bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 120, 120, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.as_numpy_iterator().next()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecde822a",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d78f01",
   "metadata": {},
   "source": [
    "To create the neural networks we are using VGG-16 as our base model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a3f4fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # input layer\n",
    "    input_layer = Input(shape=(120, 120, 3))\n",
    "    \n",
    "    # vgg16 model\n",
    "    vgg = VGG16(include_top=False)(input_layer)\n",
    "    \n",
    "    # layers for classification\n",
    "    x = GlobalAveragePooling2D()(vgg)\n",
    "    x = Dense(128)(x)\n",
    "    x = Dense(128)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    # layers for detection\n",
    "    y = GlobalAveragePooling2D()(vgg)\n",
    "    y = Dense(2048, activation=\"relu\")(y)\n",
    "    y = Dense(2048, activation=\"relu\")(y)\n",
    "    y = Dropout(0.4)(y)\n",
    "    y = Dense(4, activation=\"sigmoid\")(y)\n",
    "\n",
    "    # create the final model\n",
    "    model = Model(inputs=input_layer, outputs=[x, y])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63f76170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 120, 120, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " vgg16 (Functional)             (None, None, None,   14714688    ['input_3[0][0]']                \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " global_average_pooling2d_2 (Gl  (None, 512)         0           ['vgg16[0][0]']                  \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " global_average_pooling2d_3 (Gl  (None, 512)         0           ['vgg16[0][0]']                  \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 128)          65664       ['global_average_pooling2d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 2048)         1050624     ['global_average_pooling2d_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 128)          16512       ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 2048)         4196352     ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 128)          0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 2048)         0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 1)            129         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 4)            8196        ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 20,052,165\n",
      "Trainable params: 20,052,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define the detection model\n",
    "detection_model = build_model()\n",
    "detection_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf4dde0",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf6e6164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def localization_loss(y_true, yhat):\n",
    "    \"\"\"\n",
    "    This function is the loss function of object localization\n",
    "    \"\"\"\n",
    "    delta_coord = tf.reduce_sum(tf.square(y_true[:,:2] - yhat[:,:2]))\n",
    "                  \n",
    "    h_true = y_true[:,3] - y_true[:,1] \n",
    "    w_true = y_true[:,2] - y_true[:,0] \n",
    "\n",
    "    h_pred = yhat[:,3] - yhat[:,1] \n",
    "    w_pred = yhat[:,2] - yhat[:,0] \n",
    "    \n",
    "    delta_size = tf.reduce_sum(tf.square(w_true - w_pred) + tf.square(h_true-h_pred))\n",
    "    \n",
    "    return delta_coord + delta_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ba83eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define losses\n",
    "classification_loss = BinaryCrossentropy()\n",
    "detection_loss = localization_loss\n",
    "\n",
    "# define optimizer\n",
    "batches_per_epoch = len(train_dataset)\n",
    "learning_rate_decay = (1./0.75 - 1) / batches_per_epoch\n",
    "optimizer = Adam(learning_rate=0.0001, decay=learning_rate_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0e480b",
   "metadata": {},
   "source": [
    "Custom training loop: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9477edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDetectionModel(Model):\n",
    "    def __init__(self, trackermodel, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model = trackermodel\n",
    "\n",
    "    def compile(self, optimizer, classification_loss, detection_loss, **kwargs):\n",
    "        \"\"\"\n",
    "        This function will compile the model\n",
    "        \"\"\"\n",
    "        super().compile(**kwargs)\n",
    "        # define losses and optimizer\n",
    "        self.classification_loss = classification_loss\n",
    "        self.detection_loss = detection_loss\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    def train_step(self, batch, **kwargs):\n",
    "        \"\"\"\n",
    "        This function containes a train step of our model\n",
    "        \"\"\"\n",
    "        # define input and output datas as X and y\n",
    "        X, y = batch\n",
    "\n",
    "        # backpropegation\n",
    "        with tf.GradientTape() as tape:\n",
    "            # make model to train on classes and coordinates\n",
    "            classes, coords = self.model(X, training=True)\n",
    "            # calculate losses\n",
    "            class_loss = self.classification_loss(y[0], classes)\n",
    "            loc_loss = self.detection_loss(tf.cast(y[1], tf.float32), coords)\n",
    "\n",
    "            # calculate the total loss\n",
    "            total_loss = loc_loss  +0.5 * class_loss\n",
    "\n",
    "            # define the gradient\n",
    "            grad = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "\n",
    "        # apply gradient\n",
    "        optimizer.apply_gradients(zip(grad, self.model.trainable_variables))\n",
    "\n",
    "        return {\"total_loss\":total_loss, \"class_loss\":class_loss, \"loc_loss\":loc_loss}\n",
    "\n",
    "    def call(self, X, **kwargs): \n",
    "        return self.model(X, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "952bb073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the final model\n",
    "model = FaceDetectionModel(detection_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "de4a416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the detection model\n",
    "model.compile(optimizer, classification_loss, detection_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be76fa55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "494/494 [==============================] - 57s 101ms/step - total_loss: 0.4331 - class_loss: 0.0557 - loc_loss: 0.4052 - val_loss: 0.0000e+00\n",
      "Epoch 2/100\n",
      "494/494 [==============================] - 50s 92ms/step - total_loss: 0.0847 - class_loss: 0.0054 - loc_loss: 0.0821 - val_loss: 0.0000e+00\n",
      "Epoch 3/100\n",
      "494/494 [==============================] - 50s 93ms/step - total_loss: 0.0603 - class_loss: 0.0038 - loc_loss: 0.0584 - val_loss: 0.0000e+00\n",
      "Epoch 4/100\n",
      "494/494 [==============================] - 50s 93ms/step - total_loss: 0.0464 - class_loss: 0.0023 - loc_loss: 0.0452 - val_loss: 0.0000e+00\n",
      "Epoch 5/100\n",
      "494/494 [==============================] - 50s 93ms/step - total_loss: 0.0410 - class_loss: 0.0013 - loc_loss: 0.0404 - val_loss: 0.0000e+00\n",
      "Epoch 6/100\n",
      "494/494 [==============================] - 51s 94ms/step - total_loss: 0.0332 - class_loss: 8.3962e-04 - loc_loss: 0.0328 - val_loss: 0.0000e+00\n",
      "Epoch 7/100\n",
      "494/494 [==============================] - 51s 94ms/step - total_loss: 0.0307 - class_loss: 0.0013 - loc_loss: 0.0300 - val_loss: 0.0000e+00\n",
      "Epoch 8/100\n",
      "494/494 [==============================] - 51s 94ms/step - total_loss: 0.0272 - class_loss: 0.0010 - loc_loss: 0.0267 - val_loss: 0.0000e+00\n",
      "Epoch 9/100\n",
      "494/494 [==============================] - 51s 94ms/step - total_loss: 0.0264 - class_loss: 8.4266e-04 - loc_loss: 0.0260 - val_loss: 0.0000e+00\n",
      "Epoch 10/100\n",
      "494/494 [==============================] - 51s 94ms/step - total_loss: 0.0238 - class_loss: 9.8489e-04 - loc_loss: 0.0233 - val_loss: 0.0000e+00\n",
      "Epoch 11/100\n",
      "494/494 [==============================] - 51s 94ms/step - total_loss: 0.0220 - class_loss: 5.0799e-04 - loc_loss: 0.0218 - val_loss: 0.0000e+00\n",
      "Epoch 12/100\n",
      "494/494 [==============================] - 51s 95ms/step - total_loss: 0.0217 - class_loss: 4.7388e-04 - loc_loss: 0.0215 - val_loss: 0.0000e+00\n",
      "Epoch 13/100\n",
      "494/494 [==============================] - 51s 95ms/step - total_loss: 0.0203 - class_loss: 2.6302e-04 - loc_loss: 0.0201 - val_loss: 0.0000e+00\n",
      "Epoch 14/100\n",
      "494/494 [==============================] - 51s 95ms/step - total_loss: 0.0203 - class_loss: 4.7384e-04 - loc_loss: 0.0201 - val_loss: 0.0000e+00\n",
      "Epoch 15/100\n",
      "494/494 [==============================] - 51s 95ms/step - total_loss: 0.0191 - class_loss: 4.5329e-04 - loc_loss: 0.0189 - val_loss: 0.0000e+00\n",
      "Epoch 16/100\n",
      "494/494 [==============================] - 51s 95ms/step - total_loss: 0.0175 - class_loss: 2.4892e-04 - loc_loss: 0.0174 - val_loss: 0.0000e+00\n",
      "Epoch 17/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0187 - class_loss: 5.2531e-04 - loc_loss: 0.0184 - val_loss: 0.0000e+00\n",
      "Epoch 18/100\n",
      "494/494 [==============================] - 51s 95ms/step - total_loss: 0.0176 - class_loss: 3.5036e-04 - loc_loss: 0.0174 - val_loss: 0.0000e+00\n",
      "Epoch 19/100\n",
      "494/494 [==============================] - 51s 95ms/step - total_loss: 0.0165 - class_loss: 2.7187e-04 - loc_loss: 0.0164 - val_loss: 0.0000e+00\n",
      "Epoch 20/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0161 - class_loss: 2.4351e-04 - loc_loss: 0.0160 - val_loss: 0.0000e+00\n",
      "Epoch 21/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0162 - class_loss: 3.3238e-04 - loc_loss: 0.0160 - val_loss: 0.0000e+00\n",
      "Epoch 22/100\n",
      "494/494 [==============================] - 51s 95ms/step - total_loss: 0.0146 - class_loss: 1.1623e-04 - loc_loss: 0.0145 - val_loss: 0.0000e+00\n",
      "Epoch 23/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0139 - class_loss: 8.8148e-05 - loc_loss: 0.0138 - val_loss: 0.0000e+00\n",
      "Epoch 24/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0135 - class_loss: 7.6443e-05 - loc_loss: 0.0134 - val_loss: 0.0000e+00\n",
      "Epoch 25/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0128 - class_loss: 9.8740e-06 - loc_loss: 0.0128 - val_loss: 0.0000e+00\n",
      "Epoch 26/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0125 - class_loss: 9.5044e-06 - loc_loss: 0.0125 - val_loss: 0.0000e+00\n",
      "Epoch 27/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0124 - class_loss: 8.2128e-06 - loc_loss: 0.0124 - val_loss: 0.0000e+00\n",
      "Epoch 28/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0121 - class_loss: 8.9645e-06 - loc_loss: 0.0121 - val_loss: 0.0000e+00\n",
      "Epoch 29/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0118 - class_loss: 6.0071e-06 - loc_loss: 0.0118 - val_loss: 0.0000e+00\n",
      "Epoch 30/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0118 - class_loss: 5.8298e-06 - loc_loss: 0.0118 - val_loss: 0.0000e+00\n",
      "Epoch 31/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0113 - class_loss: 4.8940e-06 - loc_loss: 0.0113 - val_loss: 0.0000e+00\n",
      "Epoch 32/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0112 - class_loss: 3.4338e-06 - loc_loss: 0.0112 - val_loss: 0.0000e+00\n",
      "Epoch 33/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0113 - class_loss: 3.3150e-06 - loc_loss: 0.0113 - val_loss: 0.0000e+00\n",
      "Epoch 34/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0109 - class_loss: 2.6808e-06 - loc_loss: 0.0109 - val_loss: 0.0000e+00\n",
      "Epoch 35/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0108 - class_loss: 2.3676e-06 - loc_loss: 0.0108 - val_loss: 0.0000e+00\n",
      "Epoch 36/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0109 - class_loss: 2.0768e-06 - loc_loss: 0.0109 - val_loss: 0.0000e+00\n",
      "Epoch 37/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0108 - class_loss: 1.7308e-06 - loc_loss: 0.0108 - val_loss: 0.0000e+00\n",
      "Epoch 38/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0103 - class_loss: 1.4638e-06 - loc_loss: 0.0103 - val_loss: 0.0000e+00\n",
      "Epoch 39/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0106 - class_loss: 1.0704e-06 - loc_loss: 0.0106 - val_loss: 0.0000e+00\n",
      "Epoch 40/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0103 - class_loss: 1.1927e-06 - loc_loss: 0.0103 - val_loss: 0.0000e+00\n",
      "Epoch 41/100\n",
      "494/494 [==============================] - 52s 96ms/step - total_loss: 0.0102 - class_loss: 7.2829e-07 - loc_loss: 0.0102 - val_loss: 0.0000e+00\n",
      "Epoch 42/100\n",
      "494/494 [==============================] - 47s 86ms/step - total_loss: 0.0100 - class_loss: 6.0071e-07 - loc_loss: 0.0100 - val_loss: 0.0000e+00\n",
      "Epoch 43/100\n",
      "494/494 [==============================] - 47s 86ms/step - total_loss: 0.0100 - class_loss: 5.1108e-07 - loc_loss: 0.0100 - val_loss: 0.0000e+00\n",
      "Epoch 44/100\n",
      "494/494 [==============================] - 46s 85ms/step - total_loss: 0.0096 - class_loss: 4.4024e-07 - loc_loss: 0.0096 - val_loss: 0.0000e+00\n",
      "Epoch 45/100\n",
      "494/494 [==============================] - 47s 85ms/step - total_loss: 0.0097 - class_loss: 3.3197e-07 - loc_loss: 0.0097 - val_loss: 0.0000e+00\n",
      "Epoch 46/100\n",
      "494/494 [==============================] - 47s 85ms/step - total_loss: 0.0098 - class_loss: 3.4519e-07 - loc_loss: 0.0098 - val_loss: 0.0000e+00\n",
      "Epoch 47/100\n",
      "494/494 [==============================] - 47s 86ms/step - total_loss: 0.0096 - class_loss: 3.1747e-07 - loc_loss: 0.0096 - val_loss: 0.0000e+00\n",
      "Epoch 48/100\n",
      "494/494 [==============================] - 46s 85ms/step - total_loss: 0.0094 - class_loss: 2.0564e-07 - loc_loss: 0.0094 - val_loss: 0.0000e+00\n",
      "Epoch 49/100\n",
      "494/494 [==============================] - 46s 85ms/step - total_loss: 0.0094 - class_loss: 2.1050e-07 - loc_loss: 0.0094 - val_loss: 0.0000e+00\n",
      "Epoch 50/100\n",
      "494/494 [==============================] - 47s 85ms/step - total_loss: 0.0092 - class_loss: 1.7693e-07 - loc_loss: 0.0092 - val_loss: 0.0000e+00\n",
      "Epoch 51/100\n",
      "494/494 [==============================] - 47s 85ms/step - total_loss: 0.0091 - class_loss: 1.8409e-07 - loc_loss: 0.0091 - val_loss: 0.0000e+00\n",
      "Epoch 52/100\n",
      "494/494 [==============================] - 47s 86ms/step - total_loss: 0.0090 - class_loss: 8.5130e-08 - loc_loss: 0.0090 - val_loss: 0.0000e+00\n",
      "Epoch 53/100\n",
      "494/494 [==============================] - 46s 85ms/step - total_loss: 0.0090 - class_loss: 9.8566e-08 - loc_loss: 0.0090 - val_loss: 0.0000e+00\n",
      "Epoch 54/100\n",
      "494/494 [==============================] - 47s 85ms/step - total_loss: 0.0093 - class_loss: 1.2701e-07 - loc_loss: 0.0093 - val_loss: 0.0000e+00\n",
      "Epoch 55/100\n",
      "494/494 [==============================] - 47s 85ms/step - total_loss: 0.0088 - class_loss: 9.7295e-08 - loc_loss: 0.0088 - val_loss: 0.0000e+00\n",
      "Epoch 56/100\n",
      "494/494 [==============================] - 48s 88ms/step - total_loss: 0.0089 - class_loss: 7.5643e-08 - loc_loss: 0.0089 - val_loss: 0.0000e+00\n",
      "Epoch 57/100\n",
      "494/494 [==============================] - 47s 86ms/step - total_loss: 0.0087 - class_loss: 5.2643e-08 - loc_loss: 0.0087 - val_loss: 0.0000e+00\n",
      "Epoch 58/100\n",
      "494/494 [==============================] - 47s 87ms/step - total_loss: 0.0087 - class_loss: 5.3870e-08 - loc_loss: 0.0087 - val_loss: 0.0000e+00\n",
      "Epoch 59/100\n",
      "494/494 [==============================] - 46s 85ms/step - total_loss: 0.0087 - class_loss: 6.8706e-08 - loc_loss: 0.0087 - val_loss: 0.0000e+00\n",
      "Epoch 60/100\n",
      "494/494 [==============================] - 46s 85ms/step - total_loss: 0.0087 - class_loss: 6.7287e-08 - loc_loss: 0.0087 - val_loss: 0.0000e+00\n",
      "Epoch 61/100\n",
      "494/494 [==============================] - 46s 85ms/step - total_loss: 0.0088 - class_loss: 4.1285e-08 - loc_loss: 0.0088 - val_loss: 0.0000e+00\n",
      "Epoch 62/100\n",
      "494/494 [==============================] - 47s 87ms/step - total_loss: 0.0087 - class_loss: 5.4473e-08 - loc_loss: 0.0087 - val_loss: 0.0000e+00\n",
      "Epoch 63/100\n",
      "494/494 [==============================] - 52s 96ms/step - total_loss: 0.0085 - class_loss: 3.9947e-08 - loc_loss: 0.0085 - val_loss: 0.0000e+00\n",
      "Epoch 64/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0084 - class_loss: 3.9702e-08 - loc_loss: 0.0084 - val_loss: 0.0000e+00\n",
      "Epoch 65/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0085 - class_loss: 2.0889e-08 - loc_loss: 0.0085 - val_loss: 0.0000e+00\n",
      "Epoch 66/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0086 - class_loss: 2.8764e-08 - loc_loss: 0.0086 - val_loss: 0.0000e+00\n",
      "Epoch 67/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0082 - class_loss: 3.7404e-08 - loc_loss: 0.0082 - val_loss: 0.0000e+00\n",
      "Epoch 68/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0084 - class_loss: 4.7002e-08 - loc_loss: 0.0084 - val_loss: 0.0000e+00\n",
      "Epoch 69/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0083 - class_loss: 2.9682e-08 - loc_loss: 0.0083 - val_loss: 0.0000e+00\n",
      "Epoch 70/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0083 - class_loss: 2.5528e-08 - loc_loss: 0.0083 - val_loss: 0.0000e+00\n",
      "Epoch 71/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0082 - class_loss: 1.9086e-08 - loc_loss: 0.0082 - val_loss: 0.0000e+00\n",
      "Epoch 72/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0080 - class_loss: 3.1375e-08 - loc_loss: 0.0080 - val_loss: 0.0000e+00\n",
      "Epoch 73/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0082 - class_loss: 3.0171e-08 - loc_loss: 0.0082 - val_loss: 0.0000e+00\n",
      "Epoch 74/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0081 - class_loss: 2.4359e-08 - loc_loss: 0.0081 - val_loss: 0.0000e+00\n",
      "Epoch 75/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0080 - class_loss: 2.5994e-08 - loc_loss: 0.0080 - val_loss: 0.0000e+00\n",
      "Epoch 76/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0078 - class_loss: 1.9289e-08 - loc_loss: 0.0078 - val_loss: 0.0000e+00\n",
      "Epoch 77/100\n",
      "494/494 [==============================] - 51s 95ms/step - total_loss: 0.0078 - class_loss: 1.7550e-08 - loc_loss: 0.0078 - val_loss: 0.0000e+00\n",
      "Epoch 78/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0080 - class_loss: 2.0929e-08 - loc_loss: 0.0080 - val_loss: 0.0000e+00\n",
      "Epoch 79/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0081 - class_loss: 1.8343e-08 - loc_loss: 0.0081 - val_loss: 0.0000e+00\n",
      "Epoch 80/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0078 - class_loss: 1.7618e-08 - loc_loss: 0.0078 - val_loss: 0.0000e+00\n",
      "Epoch 81/100\n",
      "494/494 [==============================] - 52s 96ms/step - total_loss: 0.0078 - class_loss: 1.5631e-08 - loc_loss: 0.0078 - val_loss: 0.0000e+00\n",
      "Epoch 82/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0080 - class_loss: 1.5548e-08 - loc_loss: 0.0080 - val_loss: 0.0000e+00\n",
      "Epoch 83/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0078 - class_loss: 1.8411e-08 - loc_loss: 0.0078 - val_loss: 0.0000e+00\n",
      "Epoch 84/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0078 - class_loss: 1.5029e-08 - loc_loss: 0.0078 - val_loss: 0.0000e+00\n",
      "Epoch 85/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0077 - class_loss: 1.6732e-08 - loc_loss: 0.0077 - val_loss: 0.0000e+00\n",
      "Epoch 86/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0077 - class_loss: 1.8481e-08 - loc_loss: 0.0077 - val_loss: 0.0000e+00\n",
      "Epoch 87/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0077 - class_loss: 1.6738e-08 - loc_loss: 0.0077 - val_loss: 0.0000e+00\n",
      "Epoch 88/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0075 - class_loss: 3.0796e-08 - loc_loss: 0.0075 - val_loss: 0.0000e+00\n",
      "Epoch 89/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0076 - class_loss: 1.8619e-08 - loc_loss: 0.0076 - val_loss: 0.0000e+00\n",
      "Epoch 90/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0075 - class_loss: 1.5262e-08 - loc_loss: 0.0075 - val_loss: 0.0000e+00\n",
      "Epoch 91/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0075 - class_loss: 1.3464e-08 - loc_loss: 0.0075 - val_loss: 0.0000e+00\n",
      "Epoch 92/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0077 - class_loss: 2.9667e-08 - loc_loss: 0.0077 - val_loss: 0.0000e+00\n",
      "Epoch 93/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0074 - class_loss: 1.7106e-08 - loc_loss: 0.0074 - val_loss: 0.0000e+00\n",
      "Epoch 94/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0075 - class_loss: 1.7671e-08 - loc_loss: 0.0075 - val_loss: 0.0000e+00\n",
      "Epoch 95/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0075 - class_loss: 1.2862e-08 - loc_loss: 0.0075 - val_loss: 0.0000e+00\n",
      "Epoch 96/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0073 - class_loss: 1.2746e-08 - loc_loss: 0.0073 - val_loss: 0.0000e+00\n",
      "Epoch 97/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0073 - class_loss: 1.8536e-08 - loc_loss: 0.0073 - val_loss: 0.0000e+00\n",
      "Epoch 98/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0076 - class_loss: 1.5586e-08 - loc_loss: 0.0076 - val_loss: 0.0000e+00\n",
      "Epoch 99/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0073 - class_loss: 1.5353e-08 - loc_loss: 0.0073 - val_loss: 0.0000e+00\n",
      "Epoch 100/100\n",
      "494/494 [==============================] - 52s 95ms/step - total_loss: 0.0072 - class_loss: 1.9821e-08 - loc_loss: 0.0072 - val_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit(train_dataset, epochs=EPOCHS, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5c32251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d666dcb9",
   "metadata": {},
   "source": [
    "### Use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3c8c90c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "926d3e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "# get the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    frame = frame[50:500, 50:500, :]\n",
    "\n",
    "    # ready our frames for detection\n",
    "    rgb_images = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    resized_img = tf.image.resize(rgb_images, (120, 120))\n",
    "\n",
    "    # make predictions\n",
    "    prediction = model.predict(np.expand_dims(resized_img/255,0))\n",
    "    # get the coordinates\n",
    "    coords = prediction[1][0]\n",
    "\n",
    "    # draw square if prediction accuracy was high\n",
    "    if prediction[0] >= 0.85:\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.multiply(coords[:2], [450,450]).astype(int)),\n",
    "                      tuple(np.multiply(coords[2:], [450,450]).astype(int)), \n",
    "                            (255,0,0), 1)\n",
    "\n",
    "    # show the frame \n",
    "    cv2.imshow(\"Face detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
